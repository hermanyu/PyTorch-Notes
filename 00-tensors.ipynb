{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb5714e-db97-42c1-ba0c-c9e9f88b93af",
   "metadata": {},
   "source": [
    "# Tensor Basics\n",
    "\n",
    "Like Tensorflow, PyTorch runs on \"tensor\" objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b19f2f4-04a3-4073-9fc0-17ea313adeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99c09c34-654f-47d5-9657-a771bee5772b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an \"empty\" tensor of shape 1 (i.e. an \"empty\" scaler)\n",
    "x = torch.empty(1)\n",
    "\n",
    "# since x is empty, the tensor will hold a random number until we assign the empty space a value\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7638360b-4b45-493f-88ab-03a534afd8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.1477e-41, 0.0000e+00]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty 1 x 2 tensor (i.e. a vector)\n",
    "x = torch.empty((1,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c56985b-53a3-448d-bedf-c8618016c32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0194e-38, 9.9184e-39],\n",
       "        [2.9389e-39, 1.0194e-38],\n",
       "        [2.9389e-39, 9.2755e-39],\n",
       "        [9.0918e-39, 1.0010e-38],\n",
       "        [9.9184e-39, 1.0653e-38],\n",
       "        [9.1837e-39, 9.6428e-39],\n",
       "        [1.0010e-38, 9.1837e-39],\n",
       "        [8.9082e-39, 9.2755e-39],\n",
       "        [1.1112e-38, 0.0000e+00],\n",
       "        [0.0000e+00, 6.6533e+16]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty 10 x 2 tensor (i.e. a matrix)\n",
    "x = torch.empty((10,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abbdd092-1d4b-491e-b9b6-76b162ce6757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 9.1835e-41,  0.0000e+00],\n",
       "         [ 2.8026e-45,  0.0000e+00],\n",
       "         [-8.9940e+18,  4.5915e-41]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00],\n",
       "         [        nan,  0.0000e+00],\n",
       "         [ 1.4013e-45,  0.0000e+00],\n",
       "         [-1.7252e+15,  4.5915e-41],\n",
       "         [ 0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an empty 3 x 6 x 2 tensor (i.e. a 3-tensor)\n",
    "# Think of this as a 3-deep stack of 6 x 2 matrices.\n",
    "x = torch.empty((3,6,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d147437-0f38-48e4-8df8-881f22586fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# like numpy, we can quickly generate tensors of all 0's or all 1's\n",
    "x = torch.zeros((3, 6, 2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "77859a51-5143-46c8-ad17-96bad7cadb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((3,6,2))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61468893-ffe5-4580-96e7-28fa573cb125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also check the size of the tensor\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3ac2b-fb42-4167-87d4-3c9077b394b5",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "79a6d69b-877b-494f-bc3e-a748da78e0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 1.0000, 3.8000])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors are usually created by passing a list or np.array of values\n",
    "x = torch.tensor([2.5, 1, 3.8 ])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1a26ee27-11ae-4082-b7b6-2f0b2bcb2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5000, 1.0000, 3.8000], dtype=torch.float64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([2.5, 1, 3.8])\n",
    "\n",
    "x = torch.tensor(v)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "772db372-d9fe-4b73-8aeb-629c312a2f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2596, 0.9668, 0.5173, 0.9609],\n",
       "         [0.9325, 0.9534, 0.7186, 0.3392],\n",
       "         [0.5390, 0.3931, 0.9580, 0.0567]],\n",
       "\n",
       "        [[0.4764, 0.2889, 0.4169, 0.3076],\n",
       "         [0.0109, 0.1179, 0.9215, 0.3748],\n",
       "         [0.9160, 0.2743, 0.9339, 0.2710]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quickly generate an arbitrary tensor\n",
    "x = torch.rand(2,3,4)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "573e7bd8-5e0e-45af-a25c-32194ab96d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2596162 , 0.9668353 , 0.5172723 , 0.96090424],\n",
       "        [0.93252426, 0.9533586 , 0.7185599 , 0.33918536],\n",
       "        [0.53897727, 0.3931409 , 0.9580346 , 0.05666196]],\n",
       "\n",
       "       [[0.47643667, 0.2888782 , 0.4168514 , 0.30761725],\n",
       "        [0.01085627, 0.11785257, 0.9214798 , 0.3748451 ],\n",
       "        [0.9159908 , 0.27432448, 0.93386585, 0.27102882]]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can actually convert tensors back to numpy arrays as well\n",
    "\n",
    "# conver x to numpy array\n",
    "x= x.numpy()\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825878b-824b-4038-a748-9feb0311488b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf6f78-4280-431b-9ad1-801154c94179",
   "metadata": {},
   "source": [
    "# Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e80f6ff-6fbb-43a5-aff8-47b7aac2e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      " \n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# we can add tensors element wise\n",
    "x = torch.ones((2,3,4))\n",
    "\n",
    "y = torch.ones((2,3,4,))\n",
    "\n",
    "print(x)\n",
    "print(\" \")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d144094-264a-4a75-a439-2867209c3c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8627c36b-cf33-41fe-8e92-973bfd8b2a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + 2*y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629508a0-f37a-44a0-a02c-a15f092dfe13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd31cd3-dd3e-4260-94a9-2d925094eb83",
   "metadata": {},
   "source": [
    "# Tensor Reshaping and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ac53cf80-738f-43f0-a558-ce22c416540c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a 1D long tensor\n",
    "x = torch.tensor(list(range(0,24)))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51ca30d7-e17c-48e7-ae82-f66f7c947359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can reshape the tensor to 2 x 3 x 4\n",
    "x = x.reshape((2,3,4))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3be363ec-fe14-4fa1-a066-f2356e74b076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can select elements by slicing\n",
    "\n",
    "# select the element in the first matrix, second row, third entry\n",
    "# Note the indices start a 0, so they are all -1 from the name\n",
    "x[0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8eb904b9-2f85-4dcf-9b21-4ab5c2777811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the element in the second matrix, first row, fourth entry\n",
    "x[1,0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e74fec76-6249-465c-8e6e-aa95e84f0ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the first matrix, first two rows\n",
    "x[0, 0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66a4f5fe-7231-4e97-9c8e-3a31fe986241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3],\n",
       "        [ 6,  7],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the second matrix, last two columns\n",
    "x[0, : , 2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "413e40de-eadb-459f-b111-f6ef5c6070eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 4,  5],\n",
       "         [ 8,  9]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [16, 17],\n",
       "         [20, 21]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the first two columns across both matrices\n",
    "x[:, :, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c83bc1cf-815f-45d7-8269-4b938ab67b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6,  7],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select bottom right 2 x 2 submatrix from both matrices\n",
    "x[:, 1:3, 2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebec11-3854-4a15-bbb3-cdec9c7dd034",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb4656-a605-4030-a7e9-cc12ec8699ea",
   "metadata": {},
   "source": [
    "# CUDA and Device Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2920fe69-a4ad-459f-a481-c884ed026ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch tensors can be assigned to either the CPU or GPU\n",
    "# which decides which device the tensor computations are done on.\n",
    "x = torch.ones((2,3,4) , device = \"cuda\")\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "672bd956-d20b-4363-b3ca-461d8a31b8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3cce8c58-4bd5-44b9-aef1-cc0fe970e4be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [111]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# note: a GPU tensor cannot be converted back to a np.array.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# what we have to do instead is convert it back to a CPU tensor and then to an np.array\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# note: a GPU tensor cannot be converted back to a np.array.\n",
    "# what we have to do instead is convert it back to a CPU tensor and then to an np.array\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4d558a9-a1e6-4395-800c-cc6f09248290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert back to cpu tensor first\n",
    "x = x.to(\"cpu\")\n",
    "\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21790e-72b2-47e1-8ca2-9d3dada8db5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0e156-4168-4b20-a2d1-bb6ca043bf7c",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aebef40b-4992-411a-9d78-51e1ee1b07a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In PyTorch, gradient descent is acheived using Auto Differentiation\n",
    "\n",
    "# Auto Differentiation works by tracking each individual operation done on a tensor, then reverse engineering it\n",
    "# to compute the gradient via the chain rule (cool stuff!)\n",
    "\n",
    "# Therefore, if we need to compute the gradient of a tensor, we have to tell PyTorch to track the operations done on the tensor\n",
    "# This is done by turning on the \"requires_grad\" parameter\n",
    "x = torch.ones((2,3,4), device='cuda', requires_grad=True)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a6022f4b-9216-40b3-a3df-c47c2ec685e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do some operations\n",
    "y = x+2\n",
    "\n",
    "z = y*y*3\n",
    "\n",
    "z = z.mean()\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7891585c-94be-4836-9141-1e923c4c5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f09f9bf0-0abf-40c1-88c9-c8f22c8b2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dz/dx\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fd6891ab-9ebb-403f-98fa-302f26e743ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7500, 0.7500, 0.7500, 0.7500],\n",
       "         [0.7500, 0.7500, 0.7500, 0.7500],\n",
       "         [0.7500, 0.7500, 0.7500, 0.7500]],\n",
       "\n",
       "        [[0.7500, 0.7500, 0.7500, 0.7500],\n",
       "         [0.7500, 0.7500, 0.7500, 0.7500],\n",
       "         [0.7500, 0.7500, 0.7500, 0.7500]]], device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e75248-e74d-4b37-bffb-4335d934fbbe",
   "metadata": {},
   "source": [
    "- One thing to note: ```z.backward()``` actually computes a Jacobian matrix. Generally, we must we'll need to multiply the constructed Jacobian with a vector to get the final gradient.\n",
    "- However, since our $z$ outputs a scalar, the Jacobian matrix is already a gradient, so multiplication is required in this specific case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8e61504d-bd2e-4eb4-9ba7-3a43ad5cb9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[27., 27., 27., 27.],\n",
       "         [27., 27., 27., 27.],\n",
       "         [27., 27., 27., 27.]],\n",
       "\n",
       "        [[27., 27., 27., 27.],\n",
       "         [27., 27., 27., 27.],\n",
       "         [27., 27., 27., 27.]]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following z which is not scalar-valued\n",
    "x = torch.ones((2,3,4), device='cuda', requires_grad=True)\n",
    "y = x+2\n",
    "z = y*y*3\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7601ae0b-7668-461f-82bd-508b2d0dc650",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch1.11\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch1.11\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pytorch1.11\\lib\\site-packages\\torch\\autograd\\__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# trying to call backward() now will produce an error\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f86bc-59f6-4f06-83f9-25d9593175cc",
   "metadata": {},
   "source": [
    "So what is going on? \n",
    "- ```tensor.backward()``` differentiates the tensor ```z``` with respect to the \"leaf tensors\" aka independent variables ```x``` and stores the resulting derivative in the attribute ```x.grad``` of the leaf tensor.\n",
    "- Since we are dealing with multiple variables, the derivative comes in the form of the Jacobian, so ```tensor.backward()``` will return a Jacobian matrix.\n",
    "- When ```z``` is a scalar-valued function $z:\\mathbb{R}^n \\to \\mathbb{R}$, the Jacobian Matrix *is* the same as the gradient so everything is fine.\n",
    "- However when ```z``` is tensor-valued, the Jacobian Matrix is now a tensor of higher dimension than the original ```x``` tensor. We can't \"add\" the Jacobian to ```x```, so what we have to do is multiply it with a vector to squash the dimension down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8a666832-1e93-42e4-b25e-a3a51de40d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.ones((2,3,4), device='cuda')\n",
    "\n",
    "z.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5cc8804a-f3dd-4d1e-abe0-a0e5cfe0cbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[18., 18., 18., 18.],\n",
       "         [18., 18., 18., 18.],\n",
       "         [18., 18., 18., 18.]],\n",
       "\n",
       "        [[18., 18., 18., 18.],\n",
       "         [18., 18., 18., 18.],\n",
       "         [18., 18., 18., 18.]]], device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb423148-6ccc-4e5a-bc5e-e6e6451b5e8f",
   "metadata": {},
   "source": [
    "- Let's illustrate what is happening here. We have\n",
    "<br>\n",
    "\n",
    "$$ z = 3y^2 = 3(x+2)^2$$\n",
    "$$\\frac{\\partial z_{ijk}}{\\partial x_{ijk}} = 6(x_{ijk}+2) |_{x_{ijk}=1}$$\n",
    "$$\\frac{\\partial z_{ijk}}{\\partial x_{ijk}} = 6(1+2) = 18 $$\n",
    "\n",
    "We collect all of these into a tensor level equation:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ z = 3(x+2)^2 $$\n",
    "$$\\frac{dz}{dx} = 6(x+2) |_{x=[1]} = 6([1]+2)=[18]$$\n",
    "\n",
    "where $[1]$ and $[18]$ denote 3-tenors with those values in all entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dce0f5d3-58d2-42da-9e14-536bcbe20ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8],\n",
       "         [ 9, 10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15, 16],\n",
       "         [17, 18, 19, 20],\n",
       "         [21, 22, 23, 24]]], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see what happens when we change the values in the tensor v\n",
    "v = torch.tensor(list(range(1,25)), device='cuda').reshape((2,3,4))\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1f3ce4bc-b403-487a-bfca-a72085bae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset computational graph. This has to be done since the graph gets \"used up\"\n",
    "# everytime we call backward()\n",
    "x = torch.ones((2,3,4), device='cuda', requires_grad=True)\n",
    "y = x+2\n",
    "z = y*y*3\n",
    "\n",
    "z.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "116c4bf1-3d6d-48fd-a237-14e761c191a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 18.,  36.,  54.,  72.],\n",
       "         [ 90., 108., 126., 144.],\n",
       "         [162., 180., 198., 216.]],\n",
       "\n",
       "        [[234., 252., 270., 288.],\n",
       "         [306., 324., 342., 360.],\n",
       "         [378., 396., 414., 432.]]], device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd5b85-9711-4c97-8e0c-08eef563ff13",
   "metadata": {},
   "source": [
    "- The key thing to notice is that the tensor $v$ encodes the values we evaluate the partial derivatives to get the actual, numeric gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5b327f26-8e79-4193-8a92-0ac4fc18b7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 18.,  36.,  54.,  72.],\n",
       "         [ 90., 108., 126., 144.],\n",
       "         [162., 180., 198., 216.]],\n",
       "\n",
       "        [[234., 252., 270., 288.],\n",
       "         [306., 324., 342., 360.],\n",
       "         [378., 396., 414., 432.]]], device='cuda:0')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall that we had to re-initialize the tensor z. This is because the computational graph gets\n",
    "# used up everytime we auto differentiate, i.e. call backward()\n",
    "\n",
    "# If we want to differentiate multiple times, we'll need to tell PyTorch to save the computational graph\n",
    "x = torch.ones((2,3,4), device='cuda', requires_grad=True)\n",
    "y = x+2\n",
    "z = y*y*3\n",
    "\n",
    "# differentiate z, saving the computational graph\n",
    "z.backward(v, retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9d8a9009-5a9d-49b5-bc45-cb6f2c558681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[36., 36., 36., 36.],\n",
       "         [36., 36., 36., 36.],\n",
       "         [36., 36., 36., 36.]],\n",
       "\n",
       "        [[36., 36., 36., 36.],\n",
       "         [36., 36., 36., 36.],\n",
       "         [36., 36., 36., 36.]]], device='cuda:0')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2 = torch.ones((2,3,4), device='cuda')*2\n",
    "v3 = torch.ones((2,3,4), device='cuda')*3\n",
    "\n",
    "# clear the previous gradient stored in x\n",
    "x.grad.zero_()\n",
    "\n",
    "# differentiate z again\n",
    "z.backward(v2, retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dc7c2aa4-4d35-46ba-83f4-e2df49471ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[54., 54., 54., 54.],\n",
       "         [54., 54., 54., 54.],\n",
       "         [54., 54., 54., 54.]],\n",
       "\n",
       "        [[54., 54., 54., 54.],\n",
       "         [54., 54., 54., 54.],\n",
       "         [54., 54., 54., 54.]]], device='cuda:0')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clear gradient\n",
    "x.grad.zero_()\n",
    "\n",
    "# differentiate z once more\n",
    "z.backward(v3, retain_graph=True)\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42feafc5-7aa9-48f1-a584-2b8e844b2002",
   "metadata": {},
   "source": [
    "- Notice: we had to clear the gradient every single time we took a derivative. This is because PyTorch **accumulates gradients** in leaf tensors.\n",
    "- Why does PyTorch do this? \n",
    "    - When doing SGD, we usually train the network on mini-batches of examples.\n",
    "    - All the gradients from the min-batches must be summed together at the end in order to make one actual descent step.\n",
    "    - This is why PyTorch automatically saves the gradient information: it assumes we are going to be doing mini-match gradient descent (which will almost always be the case unless we somehow have only 200 training examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9f4ec-632e-44b5-a61e-6e175d46347d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
