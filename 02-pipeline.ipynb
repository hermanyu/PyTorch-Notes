{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b5a799-9d08-4611-9934-b6b959683cd8",
   "metadata": {},
   "source": [
    "# General Pipeline\n",
    "\n",
    "1. Design Model: decide on architecture and forward pass.\n",
    "2. Construct loss and optimizer.\n",
    "3. Training Loop.\n",
    "    - feed forward: compute prediction.\n",
    "    - backpropagation: gradients.\n",
    "    - update weigths: descend.\n",
    "\n",
    "In this notebook, we will let PyTorch take care of all 3 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac893537-cee0-48a9-b1d0-cb4d62f6419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db63e4d-4bdd-4750-aa57-1e6b43269779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([i*0.3 for i in range(0,20)], device = 'cuda', dtype=torch.float32)\n",
    "y = torch.tensor([2.2*i for i in range(0,20)], device = 'cuda', dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b619feff-fe3d-4ab3-bd8a-e60f58b85301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: weight = 1.630     ,     loss = 597.7400\n",
      "epoch 2: weight = 2.898     ,     loss = 361.5235\n",
      "epoch 3: weight = 3.884     ,     loss = 218.6557\n",
      "epoch 4: weight = 4.651     ,     loss = 132.2467\n",
      "epoch 5: weight = 5.247     ,     loss = 79.9851\n",
      "epoch 6: weight = 5.711     ,     loss = 48.3764\n",
      "epoch 7: weight = 6.072     ,     loss = 29.2589\n",
      "epoch 8: weight = 6.352     ,     loss = 17.6963\n",
      "epoch 9: weight = 6.570     ,     loss = 10.7030\n",
      "epoch 10: weight = 6.740     ,     loss = 6.4734\n",
      "epoch 11: weight = 6.872     ,     loss = 3.9152\n",
      "epoch 12: weight = 6.974     ,     loss = 2.3680\n",
      "epoch 13: weight = 7.054     ,     loss = 1.4322\n",
      "epoch 14: weight = 7.116     ,     loss = 0.8662\n",
      "epoch 15: weight = 7.164     ,     loss = 0.5239\n",
      "epoch 16: weight = 7.202     ,     loss = 0.3169\n",
      "epoch 17: weight = 7.231     ,     loss = 0.1916\n",
      "epoch 18: weight = 7.254     ,     loss = 0.1159\n",
      "epoch 19: weight = 7.272     ,     loss = 0.0701\n",
      "epoch 20: weight = 7.285     ,     loss = 0.0424\n",
      "Prediction: y = tensor([ 0.0000,  2.1856,  4.3712,  6.5568,  8.7424, 10.9279, 13.1135, 15.2991,\n",
      "        17.4847, 19.6703, 21.8559, 24.0415, 26.2271, 28.4127, 30.5983, 32.7838,\n",
      "        34.9694, 37.1550, 39.3406, 41.5262], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(0.0, device='cuda', dtype = torch.float32, requires_grad=True)\n",
    "\n",
    "# feed forward\n",
    "def forward_pass(x):\n",
    "    return (w*x)\n",
    "\n",
    "# training loop\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "# use pre-constructed loss from PyTorch\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# Use Stochastic Gradient Descent optimizer from PyTorch\n",
    "# We pass a list of weights for the optimizer to track and\n",
    "# it will take care of updating the weights for us.\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_preds = forward_pass(X)\n",
    "                            \n",
    "    # Here is where the PyTorch backend kicks in                        \n",
    "    ### compute loss\n",
    "    L = loss(y,y_preds)\n",
    "    ### compute gradient\n",
    "    L.backward()\n",
    "    ### descend!\n",
    "    optimizer.step()\n",
    "    ### reset accumlated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'epoch {epoch+1}: weight = {w:.3f}     ,     loss = {L:.4f}')\n",
    "    \n",
    "predictions = forward_pass(X)\n",
    "    \n",
    "print(f'Prediction: y = {predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897d83f-b7cf-4aa9-b01b-be982def0e9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A Neural Network \"from Scratch\"\n",
    "\n",
    "Ok, so we can now have PyTorch handle most of the actual backend training of our models. This means all we have to do is decide on the achitecture and code the feed-forward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d86eda29-0ce9-48a8-b5e7-d5d9fef60733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0681515f-1cdb-4bee-9be7-974d8fbe61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec = pd.read_csv('./data/elecdemand.csv')[0:100]\n",
    "\n",
    "X = elec[['workday', 'temp']]\n",
    "y = elec['demand']\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "Xs = torch.tensor(scale.fit_transform(X), device='cuda', dtype=torch.float32)\n",
    "y = torch.tensor(elec['demand'], device='cuda', dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4481e-c8ad-4e75-86bf-78f3968a5949",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For this exercise, we'll train a simple neural network to try and predict electricity demand using 2 input features: ```workday``` and ```temp```.\n",
    "\n",
    "Our network will have 3 layers:\n",
    "1. The input layer of size 2 units.\n",
    "2. The hidden layer of size 8 units.\n",
    "3. The output layer of size 1 unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8589103-9efe-465f-8f85-c87746c9be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss = 11.4134521484375\n",
      "epoch 2: loss = 8.952290534973145\n",
      "epoch 3: loss = 6.8200812339782715\n",
      "epoch 4: loss = 5.079548358917236\n",
      "epoch 5: loss = 3.872300386428833\n",
      "epoch 6: loss = 3.3251519203186035\n",
      "epoch 7: loss = 3.15840744972229\n",
      "epoch 8: loss = 3.0665078163146973\n",
      "epoch 9: loss = 2.9775564670562744\n",
      "epoch 10: loss = 2.8906631469726562\n",
      "epoch 11: loss = 2.800798177719116\n",
      "epoch 12: loss = 2.699164628982544\n",
      "epoch 13: loss = 2.5860092639923096\n",
      "epoch 14: loss = 2.4416728019714355\n",
      "epoch 15: loss = 2.2844529151916504\n",
      "epoch 16: loss = 2.1230156421661377\n",
      "epoch 17: loss = 1.9174059629440308\n",
      "epoch 18: loss = 1.6568453311920166\n",
      "epoch 19: loss = 1.3958898782730103\n",
      "epoch 20: loss = 1.1465448141098022\n",
      "neural network predicts: tensor([1.6621, 1.5685, 1.4748, 1.2842, 1.1739, 1.2402, 1.2402, 1.2622, 1.1516,\n",
      "        1.2402, 1.5374, 1.6307, 1.9808, 2.5235, 2.3425, 2.9759, 3.2472, 3.7145,\n",
      "        4.0915, 4.4667, 4.8437, 5.0938, 5.7209, 5.5941, 5.2823, 5.2823, 4.2782,\n",
      "        4.2165, 3.2472, 2.1617, 1.8968, 2.1165, 3.2021, 4.1531, 3.5984, 3.6528,\n",
      "        3.1113, 3.9664, 3.2971, 3.4474, 3.2971, 3.3472, 3.3973, 3.3472, 3.1113,\n",
      "        2.5686, 2.2522, 2.0713, 3.8071, 3.7795, 3.7355, 3.7190, 3.7078, 3.7119,\n",
      "        3.7475, 3.7955, 3.8073, 3.7834, 3.7955, 3.8073, 3.7955, 3.7596, 3.7357,\n",
      "        3.7135, 3.7355, 3.8126, 3.7355, 3.7795, 3.8126, 3.8456, 3.9281, 3.8566,\n",
      "        3.9476, 3.9866, 4.0649, 4.1839, 4.5569, 4.6713, 4.5569, 4.0454, 4.0062,\n",
      "        3.8814, 3.8401, 3.8401, 3.8181, 3.8015, 3.7795, 3.7465, 3.7410, 3.7245,\n",
      "        3.7104, 3.7099, 3.7083, 3.7119, 3.7596, 3.7834, 3.8433, 3.8792, 3.8910,\n",
      "        3.9391], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# randomly initialize weights\n",
    "W0 = torch.rand((2,8), device='cuda', dtype=torch.float32, requires_grad=True)\n",
    "W1 = torch.rand((8,), device='cuda', dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# design model\n",
    "def forward_pass(X):\n",
    "    z0 = torch.matmul(X,W0)\n",
    "    a1 = torch.nn.functional.relu(z0)\n",
    "    z1 = torch.matmul(a1,W1)\n",
    "    return z1\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([W0, W1], lr=learning_rate)\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # feed forward\n",
    "    y_preds = forward_pass(Xs)\n",
    "    \n",
    "    # compute loss\n",
    "    L = loss(y,y_preds)\n",
    "    \n",
    "    # backpropagate\n",
    "    L.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reset accumulation\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'epoch {epoch+1}: loss = {L}')\n",
    "    \n",
    "prediction = forward_pass(Xs)\n",
    "print(f'neural network predicts: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae1528-985e-4e18-badb-179d4b952325",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can even go ahead and see how well the model generalizes to unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02fcfff9-6aaa-466a-a030-b96ed7ee73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_test = pd.read_csv('./data/elecdemand.csv').iloc[100:150]\n",
    "\n",
    "X_test = elec_test[['workday', 'temp']]\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "Xs_test = torch.tensor(scale.fit_transform(X_test), device='cuda', dtype=torch.float32)\n",
    "y_test = torch.tensor(np.array(elec_test['demand']), device='cuda', dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb773671-45e3-430a-b746-6ecb80452ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural network predicts: tensor([1.7488, 1.7623, 1.7488, 1.7350, 1.7080, 1.7350, 1.7350, 1.7080, 1.7080,\n",
      "        1.7080, 1.6131, 1.5996, 1.4640, 1.4256, 1.5325, 1.5768, 2.0290, 2.3270,\n",
      "        2.9368, 2.0290, 1.9643, 1.8664, 1.9967, 2.4925, 2.2711, 2.2159, 2.2711,\n",
      "        2.0290, 2.2159, 2.2711, 2.3822, 2.8816, 3.3245, 2.9920, 2.9920, 2.3270,\n",
      "        2.1048, 2.0290, 1.9643, 1.7366, 1.4558, 1.4308, 1.4640, 1.5183, 4.5556,\n",
      "        4.4848, 4.3082, 4.1667, 4.0963, 3.9548], device='cuda:0',\n",
      "       grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_prediction = forward_pass(Xs_test)\n",
    "print(f'neural network predicts: {test_prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d66fd4a-a7ed-4004-afc0-b9c651820f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7060, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba57ab-e247-4571-a7ca-159adca907d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PyTorch Model\n",
    "\n",
    "Now that we can build basic neural networks, the next step is to completely outsource even the model building to the PyTorch backend. This will use PyTorch's ```nn``` API, which basically is PyTorch's version of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c03cec5-cf1c-4b7b-a1c5-f6ae6f393d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75fdc658-7c41-4806-8b2e-35a5aa1631b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch Models accept each training example as a standalone tensor, so we'll have to reshape our training data\n",
    "X = torch.tensor([[i*0.3] for i in range(0,20)], device = 'cuda', dtype=torch.float32)\n",
    "y = torch.tensor([[2.2*i] for i in range(0,20)], device = 'cuda', dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "n_samples, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2d835e4-5732-4988-a649-8d7619fe4ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (pre-trained): f(4) = 2.385\n"
     ]
    }
   ],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# instantiate a simple linear regression model\n",
    "model = nn.Linear(input_size, output_size, device='cuda')\n",
    "\n",
    "# make a prediction for an input\n",
    "X_test = torch.tensor([4], device='cuda',dtype=torch.float32)\n",
    "\n",
    "print(f'Prediction (pre-trained): f(4) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca785fc-4b39-4b2a-9830-f3d74a4693e1",
   "metadata": {},
   "source": [
    "- Just like Keras, PyTorch models are functional and can be called on inputs! This makes doing feed-forward passes much simpler to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1d7d7f6-0e76-48ae-a97f-abb64b5cee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.175870880484581\n",
      "Epoch 2: loss = 0.1452292948961258\n",
      "Epoch 3: loss = 0.13100922107696533\n",
      "Epoch 4: loss = 0.1197713240981102\n",
      "Epoch 5: loss = 0.11379070580005646\n",
      "Epoch 6: loss = 0.10897312313318253\n",
      "Epoch 7: loss = 0.10674001276493073\n",
      "Epoch 8: loss = 0.10471751540899277\n",
      "Epoch 9: loss = 0.1028987392783165\n",
      "Epoch 10: loss = 0.10127363353967667\n",
      "Epoch 11: loss = 0.10051329433917999\n",
      "Epoch 12: loss = 0.09904495626688004\n",
      "Epoch 13: loss = 0.09822092950344086\n",
      "Epoch 14: loss = 0.09690400958061218\n",
      "Epoch 15: loss = 0.09601732343435287\n",
      "Epoch 16: loss = 0.09516551345586777\n",
      "Epoch 17: loss = 0.09396514296531677\n",
      "Epoch 18: loss = 0.09304993599653244\n",
      "Epoch 19: loss = 0.09217136353254318\n",
      "Epoch 20: loss = 0.09132790565490723\n",
      "neural network predicts tensor([[ 0.5783],\n",
      "        [ 2.7326],\n",
      "        [ 4.8869],\n",
      "        [ 7.0394],\n",
      "        [ 9.1954],\n",
      "        [11.3479],\n",
      "        [13.5004],\n",
      "        [15.6529],\n",
      "        [17.8124],\n",
      "        [19.9579],\n",
      "        [22.1174],\n",
      "        [24.2769],\n",
      "        [26.4224],\n",
      "        [28.5819],\n",
      "        [30.7274],\n",
      "        [32.8869],\n",
      "        [35.0465],\n",
      "        [37.2060],\n",
      "        [39.3374],\n",
      "        [41.4970]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# since our weights are now instantiated by nn.Linear(), we have to pass those to the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # feed forward, just call the model!\n",
    "    y_preds = model(X)\n",
    "    \n",
    "    # compute loss\n",
    "    L = loss(y, y_preds)\n",
    "    \n",
    "    # backprop\n",
    "    L.backward()\n",
    "    \n",
    "    # descend!\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: loss = {L}')\n",
    "    \n",
    "pred = model(X)\n",
    "print(f'neural network predicts {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402e0e4-78d0-4091-ad95-8a8e4c909e22",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Custom Models\n",
    "\n",
    "We can package all of the steps above into a custom class, we just have to subtype from the ```nn.Module``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fffb0d3e-e2d1-455c-b96c-ad07427ea1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(LinearReg, self).__init__()\n",
    "        # define layers here\n",
    "        self.lin = nn.Linear(input_dim, output_dim, **kwargs)\n",
    "        \n",
    "    # we have to build the forward pass so we can call the model\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed77d724-7802-4ee9-bc36-79d95e030fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[i*0.3] for i in range(0,20)], device = 'cuda', dtype=torch.float32)\n",
    "y = torch.tensor([[2.2*i] for i in range(0,20)], device = 'cuda', dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "n_samples, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ddf2cc17-c4af-48f0-be34-1a5590eff422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 590.0912475585938\n",
      "Epoch 2: loss = 343.4002380371094\n",
      "Epoch 3: loss = 199.92520141601562\n",
      "Epoch 4: loss = 116.5318603515625\n",
      "Epoch 5: loss = 67.75375366210938\n",
      "Epoch 6: loss = 39.60836410522461\n",
      "Epoch 7: loss = 23.197452545166016\n",
      "Epoch 8: loss = 13.60979175567627\n",
      "Epoch 9: loss = 8.003260612487793\n",
      "Epoch 10: loss = 4.754348278045654\n",
      "Epoch 11: loss = 2.8956503868103027\n",
      "Epoch 12: loss = 1.7933361530303955\n",
      "Epoch 13: loss = 1.1368954181671143\n",
      "Epoch 14: loss = 0.773699939250946\n",
      "Epoch 15: loss = 0.5494605302810669\n",
      "Epoch 16: loss = 0.4306178689002991\n",
      "Epoch 17: loss = 0.34970998764038086\n",
      "Epoch 18: loss = 0.3105721175670624\n",
      "Epoch 19: loss = 0.2798733413219452\n",
      "Epoch 20: loss = 0.26517897844314575\n",
      "neural network predicts tensor([[ 0.9373],\n",
      "        [ 3.0541],\n",
      "        [ 5.1708],\n",
      "        [ 7.2858],\n",
      "        [ 9.4043],\n",
      "        [11.5193],\n",
      "        [13.6344],\n",
      "        [15.7494],\n",
      "        [17.8713],\n",
      "        [19.9795],\n",
      "        [22.1014],\n",
      "        [24.2233],\n",
      "        [26.3314],\n",
      "        [28.4534],\n",
      "        [30.5615],\n",
      "        [32.6834],\n",
      "        [34.8053],\n",
      "        [36.9272],\n",
      "        [39.0216],\n",
      "        [41.1435]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# instantiate our \"custom\" model\n",
    "model = LinearReg(input_size, output_size, device='cuda')\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # feed forward, just call the model!\n",
    "    y_preds = model(X)\n",
    "    \n",
    "    # compute loss\n",
    "    L = loss(y, y_preds)\n",
    "    \n",
    "    # backprop\n",
    "    L.backward()\n",
    "    \n",
    "    # descend!\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: loss = {L}')\n",
    "    \n",
    "pred = model(X)\n",
    "print(f'neural network predicts {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d6557-f613-4112-9b99-f384e099fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
