{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65925990-7eac-4b24-a17e-7abef20a7a3e",
   "metadata": {},
   "source": [
    "# Loss and Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fa7fb1-f5ef-495e-821e-94021742f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a98743-99b0-428f-85e8-312ccbae2aef",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "It's finally time to start talking about loss and activation (which will be the final piece of info we need to start building actual models).\n",
    "\n",
    "For neural networks, there are 3 basic loss functions which are used based on the task at hand:\n",
    "1. **Regression:** uses Mean Squared Error (MSE) as loss.\n",
    "2. **Binary Classification:** uses Binary Cross-Entropy (BCE) as loss.\n",
    "3. **Multi-Class Classification:** uses Cross-Entropy (CE) as loss.\n",
    "\n",
    "Each of these loss functions can be called from the ```torch.nn``` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf293bf-ffd0-4951-8240-31a8fef4ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "ce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a78dee-eae8-4076-95a0-d4540ef60b52",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can also define custom loss functions too, if we need them train a specific kind of network for a specific task. Usually, this means for Generative Deep Learning models like GANs and VAEs.\n",
    "\n",
    "A loss function is just a function (duh!) but the key thing to keep in mind is that we eventually have to differentiate this function using PyTorch's autograd software, so we need to make sure all the operations of the loss function are compatible with the Torch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e42a9a-26c8-4fe5-bf5c-38b59ed1b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_quartic_error(y_pred_batch, y_batch):\n",
    "    return (y_pred_batch - y_batch)**4/y_batch.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ec525-33ff-437b-9845-bcdbfbd2b1ce",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's test out PyTorch's built-in Cross Entropy Loss. The thing to note is that ```nn.CrossEntropyLoss()``` automatically:\n",
    "1. has a built-in softmax activation at the end, so no need to call one on our own.\n",
    "2. automatically matches indices of the multi-class, so we **DO NOT** one-hot encode the y labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c512ef-1b7d-4f27-9803-8c7fac416464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061220210045576096\n",
      "3.786224603652954\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# create an output whose class is \"0\"\n",
    "y = torch.tensor([0], device='cuda')\n",
    "\n",
    "# create an example of a \"good\" prediction that predicts class \"0\" with high confidence\n",
    "y_pred_good = torch.tensor([[4.0, 0.2, 0.8]], device='cuda')\n",
    "\n",
    "# create an example of a \"bad\" prediction that does not predict \"0\"\n",
    "y_pred_bad = torch.tensor([[0.1, 3.6, 2.4]], device='cuda')\n",
    "\n",
    "# compute the loss for the good and bad predictions.\n",
    "# note that the loss function returns a tensor\n",
    "loss_good = loss(y_pred_good, y)\n",
    "loss_bad = loss(y_pred_bad, y)\n",
    "\n",
    "# extract the scalar value from the returned tensor\n",
    "print(loss_good.item())\n",
    "print(loss_bad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7e1893c-9328-4e0f-861a-65512907ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# we can also turn the logit tensors to actual weights (logit = \"probability\" though unnormalized)\n",
    "_, predictions_good = torch.max(y_pred_good, dim = 1)\n",
    "_, predictions_bad = torch.max(y_pred_bad, dim=1)\n",
    "\n",
    "print(predictions_good.item())\n",
    "print(predictions_bad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1d8df-6534-48bc-a039-56e2bab0871c",
   "metadata": {},
   "source": [
    "- The good prediction predicts class \"0\".\n",
    "- The bad prediction predicts class \"1\".\n",
    "\n",
    "Notice that the y label is not encoded as a one-hot matrix. Let's see how this works by looking at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df79735-891b-43d5-8050-bd93f42f397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02796681597828865\n",
      "9.5007905960083\n"
     ]
    }
   ],
   "source": [
    "# create an output whose class is \"7\" out of possible classes 0-9\n",
    "y = torch.tensor([7], device='cuda')\n",
    "\n",
    "# create an example of a \"good\" prediction that predicts class \"7\" with high confidence\n",
    "y_pred_good = torch.tensor([[0.1, 0.01, 0.01, 0.8, 0.2, 0.05, 0.4, 6, 0.2,0.1]], device='cuda')\n",
    "\n",
    "# create an example of a \"bad\" prediction that does not predict \"7\"\n",
    "y_pred_bad = torch.tensor([[0.4, 9.8, 0.01, 0.8, 1.2, 0.05, 0.4, 0.3, 0.2,0.1]], device='cuda')\n",
    "\n",
    "# compute the loss for the good and bad predictions.\n",
    "# note that the loss function returns a tensor\n",
    "loss_good = loss(y_pred_good, y)\n",
    "loss_bad = loss(y_pred_bad, y)\n",
    "\n",
    "# extract the scalar value from the returned tensor\n",
    "print(loss_good.item())\n",
    "print(loss_bad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfaef1fc-02d1-406a-af0b-7c9f9990d391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "_, predictions_good = torch.max(y_pred_good, dim = 1)\n",
    "_, predictions_bad = torch.max(y_pred_bad, dim=1)\n",
    "\n",
    "print(predictions_good.item())\n",
    "print(predictions_bad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828f92f-4fcf-4e04-9bb4-d6f2a359f121",
   "metadata": {},
   "source": [
    "Basically: in Keras/Tensorflow, the ```CategoricalCrossEntropy()``` loss has one-hot encoded target to organize classes. In PyTorch, the ```CrossEntropyLoss()``` uses class-index labelling to organize classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c757d-e157-49fe-a6c0-e4506136636e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "Now its time for the final piece of the puzzle: activation functions. Activation functions add non-linearity to the network allowing it to approximate more complicated shapes. In fact, the beauty of neural networks comes from the fact that mixing in a simple activation function to all the otherwise linear operations is enough to approximate almost every continuous function (Universal Approximation Theorem).\n",
    "\n",
    "The most commonly used activation functions are:\n",
    "1. ```Sigmoid()``` function. This was the forerunner to all activation functions, used for logistic regression which eventually evolved into neural networks.\n",
    "2. ```Softmax()``` function (soft max). The softmax function pools multiple values together and normalizes them into a probability. Mainly used for multi-class problems, although PyTorch's cross entropy loss already does this for us.\n",
    "3. ```Tanh()``` function (hyperbolic tangent). Similar in shape to the sigmoid, mainly used with nowadays with GANs and other generative models.\n",
    "4. ```ReLU()``` function (Rectified Linear Unit). Created to address the vanishing and exploding gradient problem with the Sigmoid and Tanh functions. However, since the ReLU function is 0 for all $x<=0$, this causes an issue with \"dying neurons\" as neurons that hit a negative value can no longer contribute anything to the gradient, thereby staying dead forever.\n",
    "5. ```LeakyReLU()``` function (\"Leaky\" ReLU). Created to address the dying neuron problem of ReLU. The idea is to take the ReLU function and slightly alter the negative-branch so that it does not stay 0 forever, allowing the gradient to \"leak through\" to the dead neuron, bringing it back to life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27dbcc48-d69b-4390-866d-b103aa5ccea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate sigmoid\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# hyperbolic tangent\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "# ReLU\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Leaky ReLU\n",
    "leaky = nn.LeakyReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65621dc2-3df3-4161-bf7d-37177f98dd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
